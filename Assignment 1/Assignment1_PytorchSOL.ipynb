{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UZonrA82NegH"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG-6oe-RNiuL"
   },
   "source": [
    "# Assignment 1b - Introduction to PyTorch\n",
    "\n",
    "In this assignment, you will be going over the basics of PyTorch as covered in the notes and the slides. \n",
    "\n",
    "Some background about Pytorch. PyTorch is, like TensorFlow, an open-source machine learning library. Contraty to TensorFlow, PyTorch is created mainly by Facebook's AI lab. PyTorch is nice and efficient due to its optimization concerning tensor like computation (as Numpy and TF) and also due to it being specifically targeted for deep learning applications.\n",
    "\n",
    "Before we get started, double check that your torch version is up to date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "msFv9cx2IAUt",
    "outputId": "d6f4791e-5903-447e-8549-6ee17db32c8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3W4OitykQc_4"
   },
   "source": [
    "Also, don't forget to turn on your **GPU** through *runtime* > *change runtime type* > *GPU*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivHhtuG-HlzQ"
   },
   "source": [
    "# 1- Data Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHFLAEQDaoiQ"
   },
   "source": [
    "In PyTorch, you have to write your own custom **datasets**, **dataloaders**. Each of them, as expected, are encapsulated into a different class. The **dataset** class needs to be initialized as follows:\n",
    "  - initializer with path to data/data, transforms*\n",
    "  - len: length of the dataset\n",
    "  - __getitem__(idx): obtaining a data sample at index `idx`.\n",
    "\n",
    "Dataloaders are extremely easy to be intialized once you have created your custom dataset through `DataLoader(dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)`\n",
    "\n",
    "You will practice more with this during your own projects, so we will save this part for you. Take a look at this and understand what each part does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dkElJhYBNguk"
   },
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, labels, images, transform=None):\n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.images[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLVKej0Pb5Cv"
   },
   "source": [
    "Transforms is for data augmentation. Luckily, PyTorch has a range of functions defined for us to perform augmentation, so we don't have to do it!\n",
    "\n",
    "**Question 1** Find a function for the following data augmentations from `transforms` and explain how many/what arguments it takes:\n",
    "\n",
    "- Convert image from RGB to Grey\n",
    "- Randomly performs a crop in the middle \n",
    "- Converts to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions**\n",
    "- `transforms.Grayscale(output_channels)`\n",
    "- `RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')`\n",
    "- `toTensor()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkGoGu3AHLPc"
   },
   "source": [
    "# 2 - Defining Neural Networks\n",
    "\n",
    "Neural Networks are encapsulated into a *class*, since, as discussed earlier, PyTorch focused a lot on the object oriented part of Python. Generally, a `class Network` at least contains the following functions:\n",
    "  - an initializer with no variables\n",
    "  - a `forward(input)` function that basically resembles `__call__` and calculates the output of running the network on `input`.\n",
    "  - (optional) `num_flat_features(x)` function that returns the number of parameters that you have if you flatten a specific x.\n",
    "\n",
    "**Question 2** Complete the code below such that it satisfies the following neural network architecture:\n",
    "![Neural Network](https://www.researchgate.net/profile/Angel_Cruz-Roa/publication/263052166/figure/fig3/AS:614373047402509@1523489357882/3-layer-CNN-architecture-composed-by-two-layers-of-convolutional-and-pooling-layers-a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NsOV_AJESA9Q"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # BEGIN CODE\n",
    "        ...\n",
    "        # END CODE\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BEGIN CODE\n",
    "        ...\n",
    "        # END CODE\n",
    "\n",
    "    # OPTIONAL\n",
    "    def num_flat_features(self, x):\n",
    "        # BEGIN CODE\n",
    "        ...\n",
    "        # END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(3, 16, 8)\n",
    "        self.conv2 = nn.Conv2d(3, 32, 8)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(32 * 15 * 15, 128)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x)) # OPTION A\n",
    "        # x = x.view(-1, 32 * 15 * 15) # OPTION B\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Npk4S0vAHEL4"
   },
   "source": [
    "# 3 - Tensors, Dimensionality and Numpy\n",
    "\n",
    "Tensors are like numpy arrays, a data structure where all the `dtypes` are the same. Across all libraries `TF`, `numpy` and `torch`, they have the same functionality, namely representing vectors, through which we can do fast computation. Contrary to **tensorflow**, you will not have to juggle with *constants* and *placeholder variables*. It is created to be for Object Oriented Programming and behaves a lot more like the arrays you are used from Numpy. Let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwzWXLgBH9HV",
    "outputId": "330e267c-79d9-4efc-cdbd-312875f33b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 1.8788e+31],\n",
      "        [1.7220e+22, 4.7881e+22, 6.7200e-07],\n",
      "        [1.0141e-11, 8.1929e-10, 3.3234e-09],\n",
      "        [5.4363e+22, 8.4945e+20, 8.3564e+20],\n",
      "        [2.6948e-09, 3.1369e+27, 7.0800e+31]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U3IAPzJJm3O"
   },
   "source": [
    "**Question 3.1** What do you notice about `torch.empty`, every time we initialize an empty tensor? Are the values random? What can you find online about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYtXxeYDJval"
   },
   "source": [
    "**Solution**: As we can see on [torch documentation](https://pytorch.org/docs/stable/generated/torch.empty.html, `empty` takes uninitialized, consecutive data blocks and then creates a tensor from those. That's why you see random values; these are basically garbage values. If we want to be sure that nothing is happening we would rather use other functions as we demonstrate in the demo below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XD-e5AtKdFr"
   },
   "source": [
    "As you noticed, the values of `empty` change every time! Let's look at other functions, which look really familiar to Numpy functions, that take care for us of that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgbxIao8K-q2"
   },
   "source": [
    "**Question 3.2** Find the equivalents for the following numpy functions in torch:\n",
    "\n",
    "- `np.ones((5,3))`\n",
    "- `np.random.randn((10,1))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHJF50RSK9_v",
    "outputId": "b73db100-8d36-45c1-bcf2-73e765422487"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "torch.ones((5,3));\n",
    "torch.rand((10, 1));\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fbPuNbqLtqS"
   },
   "source": [
    "To obtain the dimensionality of your tensors, just simply do `tensor.Size()` and you will get a pretty familiar sight as you have learned in numPy and the TensorFlow tutorials.\n",
    "\n",
    "Let's see into the differences between `PyTorch tensors` and `TensorFlow tensors`. \n",
    "\n",
    "**Question 3.3** Change the values on the diagonal to be `1`, `2`, `3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rrvq0v_HMYMr",
    "outputId": "565f3693-5e0d-4461-d3fd-f92ff4616cd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[999., 999., 999.],\n",
       "        [999., 999., 999.],\n",
       "        [999., 999., 999.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT TEMPER WITH THIS CELL\n",
    "myTensor = torch.from_numpy(999*np.ones((3,3)))\n",
    "myTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ga3t8-_mM1CP",
    "outputId": "76102a44-d64c-4d96-86f7-e0cf653d1680"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 999., 999.],\n",
       "        [999.,   2., 999.],\n",
       "        [999., 999.,   3.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START SOLUTION\n",
    "myTensor[0,0] = 1\n",
    "myTensor[1,1] = 2\n",
    "myTensor[2,2] = 3\n",
    "myTensor\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xrgC4weMXtR"
   },
   "source": [
    "Way easier than TensorFlow right? That is because PyTorch is designed to be a object oriented program and therefore, also to allow for easy mutations. We will not be going over the following trivial functions:\n",
    "\n",
    "- `torch.from_numpy()`\n",
    "- `tensor.numpy()`\n",
    "\n",
    "*Why are we using Tensors though? Let's take a look at the demo below.* Think about what happens if differences of millions of parameters build up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMVkc6ZpNpey",
    "outputId": "9d95ac49-6dc2-426c-e633-111a178b3fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.82 s, sys: 1.22 s, total: 4.04 s\n",
      "Wall time: 4.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#NUMPY\n",
    "x = np.random.rand(10000, 10000)\n",
    "y = np.random.rand(10000, 10000)\n",
    "a = x + y\n",
    "b = x - y\n",
    "c = x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_5J952qJln8",
    "outputId": "dd7b5dd9-29f6-4102-90c7-93bc469ccf91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.25 s, sys: 738 ms, total: 1.99 s\n",
      "Wall time: 2.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TENSOR\n",
    "x = torch.rand(10000, 10000)\n",
    "y = torch.rand(10000, 10000)\n",
    "a = x + y\n",
    "b = x - y\n",
    "c = x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pxmmpn1LQRwK"
   },
   "source": [
    "## GPU: Cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd3OjRLGOsEu"
   },
   "source": [
    "We can also use our GPUs! Observe the demo below. Basically, we can use GPUs through defining a `device` and then sending our tensors to `device`. To extract values, we want to extract the data from the `device-type`.\n",
    "\n",
    "*HINT* Make sure your GPU is turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsRbTKbLPC-B"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")          # a CUDA device object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xm17WNWOqMI",
    "outputId": "1d5535f3-2a85-446d-b401-2ae71004c7e0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = torch.rand(10000, 10000, device=device)\n",
    "y = torch.rand(10000, 10000, device=device)\n",
    "x.to(device) # send data\n",
    "y.to(device) # send data\n",
    "a = x + y\n",
    "b = x - y\n",
    "c = x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fU31x4KHHPZ"
   },
   "source": [
    "# 4 - Mathematical Operations and Gradients\n",
    "\n",
    "Unlike in TensorFlow, we can just use the arithmetic operators in python: +, -., *, **, /, //, % etc. to perform calculations, which is really nice, because it saves a lot of complexities. In this short passage, we will be covering the calculation of gradients because also that is optimized in the PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z47Tny0BRhPT"
   },
   "source": [
    "To be able to calculate a gradient in PyTorch, you need to initialize a tensor as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4aN0c9MRgpm",
    "outputId": "5b475581-da83-434f-cfa2-03e6ac39dd45"
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiWmc7NsXgDN"
   },
   "source": [
    "Let's take a look at what happens when we perform linear transformations on x. What happens to the gradient designation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MW3dMjxdOSYU",
    "outputId": "1abf3287-0343-4a90-b1a0-da898acd6a15"
   },
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfnQ75UHX_FE",
    "outputId": "079b1c3f-6b7a-4126-c6eb-d24ee5d6d355"
   },
   "outputs": [],
   "source": [
    "y.backward(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnHwB8tSYFy8"
   },
   "source": [
    "Does that make sense to be its partial derivative $\\dfrac{dY}{dX}$? It does right!\n",
    "\n",
    "**Question 4** Imagine, we don't want to track gradients anymore. Look up on the PyTorch documentation on how to get a copy of y without tracking gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MCYoZ4pRYDYf"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "y_notrack = y.detach()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtodBIPDHVWl"
   },
   "source": [
    "# 5 - Optimizers, Functions (Torch-Specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlZS-Tw_Y6ZU"
   },
   "source": [
    "All functions that you will be using have been pre-defined in the [nn library](https://pytorch.org/docs/stable/nn.html). \n",
    "\n",
    "**Question 5.1** How do we combine multiple functions sequentially? Look up a function that allows you to combine first a `ReLu` and then a `Softmax` into one function. Assign this function to variable `myfunc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUUs_0igY2zl"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "my_func = nn.Sequential(nn.ReLU(), nn.Softmax())\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h51_VYdQZlzW"
   },
   "source": [
    "We also have **optimizers** and **learning schedulers**. These are important for how you want to do your optimization for your neural network: `torch.optim`. The demo below is not supposed to be run but you will see more in depth how this works in assignment 2b!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SHUZBx5ZlRw"
   },
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# for input, target in dataset:\n",
    "#     def closure():\n",
    "#         optimizer.zero_grad() # Zero out gradients\n",
    "#         output = model(input) # Push input through\n",
    "#         loss = loss_fn(output, target) # Calculate loss\n",
    "#         loss.backward() # Computer Backpropagation\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKrpR-Gyc4XU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPJoNaHn7E/y7dB0Aiv17eV",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "assignment1b.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
