{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width = 130 height = 130 align = left src=\"tfkeras.jpg\">\n",
    " \n",
    "# Tensorflow for Deep Learning \n",
    "\n",
    "Learning Objectives: *By the end of this assignment, you should be comfortable with using Keras Sequential and Functional APIs for constructing deep learning models. You should be comfortable with debugging common modeling errors and researching Tensorflow documentation for various open-ended tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras** is a deep learning API that runs on top of Tensorflow, with **layers** and **models** as the core data structures. In Tensorflow 2.0, modeling functionalities are organized under the Keras namespace. (Optional: read about v1 --> v2 API cleanup [here](https://github.com/tensorflow/community/blob/master/rfcs/20180827-api-names.md))\n",
    "\n",
    "Keras provides a clean, approachable interface with abstractions and building blocks for easy prototyping and modeling customizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow_version works only in colab\n",
    "try: \n",
    "    %tensorflow_version 2.x\n",
    "except Exception: \n",
    "    pass\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several examples use the mnist dataset -- hence import & split into train/valid/test sets\n",
    "\n",
    "\"\"\"\n",
    "Background on Fashion-MNIST Dataset: \n",
    "Fashion-MNIST is a dataset of Zolando's article images, containing 60k training samples and 10k test samples.  \n",
    "Each 28x28 greyscale image belongs to one of ten classes (t-shirt/top, trouser, pullover, dress, coat, sandal, \n",
    "shirt, sneaker, bag, ankle boot). Each pixel-value is an integer between 0 and 255, where higher means darker.\n",
    "\"\"\"\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train_full, y_train_full), (x_test,  y_test) = mnist.load_data()\n",
    "x_train_full = x_train_full.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "# amount of images to be in the validation set (80/20 split)\n",
    "split_amt = int(len(x_train_full) * .2)\n",
    "\n",
    "x_train, x_valid = x_train_full[split_amt:] / 255.0, x_train_full[:split_amt] / 255.0\n",
    "y_train, y_valid = y_train_full[split_amt:], y_train_full[:split_amt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequential API - Basics\n",
    "\n",
    "The Sequential API allows one to construct the simplest type of model: one with a linear stock of layers -- ie. layers created in a step by step fashion. \n",
    "\n",
    "In the example below, we are interested in constructing a model for 10-class classification with the Fashion-MNIST dataset. Carefully examine the code and associated comments below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# creates a list of layer definitions\n",
    "seq_model = Sequential([ \n",
    "    # generate 16 3x3 filters & relu activation \n",
    "    Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)), \n",
    "    # 2x2 max-pooling -- every four pixels, the largest value survives\n",
    "    MaxPooling2D(2, 2), \n",
    "    # generate 32 3x3 filters & relu activation \n",
    "    Conv2D(32, (3, 3), activation=\"relu\"), \n",
    "    # 2x2 max-pooling -- every four pixels, the largest value survives\n",
    "    MaxPooling2D(2, 2), \n",
    "    # flattens to 1D array \n",
    "    Flatten(), \n",
    "    # fully connected hidden layer with 256 neurons & relu activation\n",
    "    Dense(256, activation=\"relu\"), \n",
    "    # fully connected hidden layer with 128 neurons & relu activation \n",
    "    Dense(128, activation=\"relu\"), \n",
    "    # fully connected output layer with 10 neurons & softmax activation \n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# displays model layers (+ layer (type), output shape, param #)\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step following model instantiation is to call the *compile()* method and specify a loss function, optimizer, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                  # loss -- String (name of objective function), objective function, or Loss instance.\n",
    "seq_model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                  # optimizer -- String (name of optimizer) or optimizer instance. \n",
    "                  optimizer = \"sgd\", \n",
    "                  # list of metrics to be evaluated by the model during training and testing.\n",
    "                  # each can be a String (name of built-in function), function, or Metric instance. \n",
    "                  metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step following compiling is to train the model by calling the *fit()* method. <br> Doing so returns a *History* object, with its *History.history* attribute holding records of training loss and metrics values at every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            # input data\n",
    "seq_history = seq_model.fit(x = x_train, \n",
    "                            # input labels\n",
    "                            y = y_train, \n",
    "                            # epoch -- an iteration over the entire x and y dataset provided\n",
    "                            epochs = 2, \n",
    "                            # data on which to evaluate the loss and any model metrics at the ennd of each epoch\n",
    "                            validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, fitting the \"best\" model may take a number of iterations -- possibly needing several hyperparameter adjustments! Once complete, the final step is to evaluate the model using the *evaluate()* method on the test set. Recall that the model is evaluated only ONCE on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns loss value & metrics values for model in test mode (default batchsize is 32)\n",
    "test_loss, test_accuracy = seq_model.evaluate(x_test, y_test)\n",
    "print(f'train loss: {test_loss:.3f}')\n",
    "print(f'train accuracy: {100 * test_accuracy:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the general steps to constructing a model using the Sequential API are: <br> \n",
    "1. Sequential instantiation, with a list of layers\n",
    "2. Compiling the model, indicating the desired loss function, optimizer, and metric(s)\n",
    "3. Fitting the model with the dataset\n",
    "4. Evaluating the model on the test set\n",
    "\n",
    "While the Sequential API is easy to use, we cannot create models that share layers, have branches, nor have multiple inputs/outputs. However, we *can* with the Functional API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: Part 1 Questions\n",
    "You may consult Tensorflow documentation and online sources for any of the questions below. <br>\n",
    "a) Describe the architecture of *seq_model* above. (ie. how many/what types of layers? what is a dense? flatten?) <br>\n",
    "\n",
    "*your answer here*\n",
    "\n",
    "b) What happens if you do not specify an activation function for any one of the dense layers? <br>\n",
    "\n",
    "*your answer here*\n",
    "\n",
    "c) The three dense layers in *seq_model* have 200960, 32896, and 1290 trainable parameters respectively. Based on your knowledge of densely connected neural networks, explain how these numbers are derived. <br> \n",
    "\n",
    "*your answer here*\n",
    "\n",
    "d) Why is the loss function sparse categorical crossentropy rather than just categorical crossentropy? <br>\n",
    "\n",
    "*your answer here*\n",
    "\n",
    "e) What happens if you do not specify a number of epochs in *fit()*? <br>\n",
    "\n",
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functional API - Basics\n",
    "\n",
    "*func_model* below contains the same architecture as *seq_model*, but uses the Functional API. Carefully examine the code below and read the comments. Compare it with the Sequential syntax above. What differences and similarities do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model \n",
    "\n",
    "# define input tensor\n",
    "input_layer = Input(shape = (28, 28, 1))\n",
    "\n",
    "# stack layers using the syntax: current_layer()(previous_layer)\n",
    "conv1 = Conv2D(16, (3, 3), activation=\"relu\")(input_layer)\n",
    "mp1 = MaxPooling2D(2, 2)(conv1)\n",
    "conv2 = Conv2D(32, (3, 3), activation=\"relu\")(mp1)\n",
    "mp2 = MaxPooling2D(2, 2)(conv2)\n",
    "flattened = Flatten()(mp2)\n",
    "fc1 = Dense(256, activation = \"relu\")(flattened)\n",
    "fc2 = Dense(128, activation = \"relu\")(fc1)\n",
    "predictions = Dense(10, activation = \"softmax\")(fc2)\n",
    "\n",
    "# define model object -- specify input and outputs\n",
    "func_model = Model(inputs = [input_layer], outputs = [predictions])\n",
    "\n",
    "# displays model layers (+ layer (type), output shape, param #)\n",
    "func_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Sequential model, we call the *compile()* method and specify a loss function, optimizer, and metrics. Then, call the *fit()* method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_model.compile(loss = \"sparse_categorical_crossentropy\", \n",
    "                   optimizer = \"sgd\", \n",
    "                   metrics = [\"accuracy\"])\n",
    "\n",
    "func_history = func_model.fit(x_train, \n",
    "                              y_train, \n",
    "                              epochs = 2, \n",
    "                              validation_data = (x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns loss value & metrics values for model in test mode (default batchsize is 32)\n",
    "test_loss, test_accuracy = func_model.evaluate(x_test, y_test)\n",
    "print(f'train loss: {test_loss:.3f}')\n",
    "print(f'train accuracy: {100 * test_accuracy:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the general steps to constructing a model using the Functional API are: <br>\n",
    "1. explicitly defining the input layer\n",
    "2. defining model layers, connecting each layer using Python functional syntax\n",
    "3. defining the model by calling the model object and giving it the input and output layers\n",
    "4. compiling --> fitting (several iterations) --> evaluating\n",
    "\n",
    "\n",
    "#### TO DO: Part 2 Questions \n",
    "You may consult Tensorflow documentation and online sources for the questions below. <br>\n",
    "a) In the context of modeling, what are the advantage(s) of using the functional syntax? <br>\n",
    "\n",
    "*your answer here*\n",
    "\n",
    "b) Notice that when defining the model object, the parameter names are plural (inputs vs input, outputs vs output). Why is this the case? <br>\n",
    "\n",
    "*your answer here*\n",
    "\n",
    "c) Refit either *seq_model* or *func_model*, but with epochs = 15. Then evaluate the corresponding metrics in the following cell and plot two double-line graphs. You should color code your lines and import any necessary libraries: <br>\n",
    "1. train & validation loss for every epoch \n",
    "2. train & validation accuracy for every epoch  \n",
    "\n",
    "Based on your graph(s), at how many epochs would you stop training? ( in section 3, you will work with callbacks for early stopping once a desired metric value is reached!) <br>\n",
    "\n",
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c -- select one\n",
    "seq_metrics = seq_history.history\n",
    "func_metrics = func_history.history\n",
    "\n",
    "print(type(seq_metrics), type(func_metrics))\n",
    "\n",
    "\"\"\" ### your code below ### \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting your model with a specified number of epochs, you can automate the prevention of further training when certain conditions are met -- ex. when accuracy or loss reaches a certain threshold, when accuracy hasn't improved after a specified number of epochs, etc. \n",
    "\n",
    "The training loop supports *callbacks* such that after every epoch, there is a callback to a code function that evaluates your metrics and decides whether to continue or stop training. \n",
    "\n",
    "Carefully examine the code and associated comments below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports abstract base class used to build new callbacks\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class myCallback(Callback): \n",
    "    # on_epoch_end -- called whenever an epoch ends\n",
    "    # sends a log object that contains information about the current training state\n",
    "    def on_epoch_end(self, epoch, logs = {}): \n",
    "        # queries the training accuracy metric, checking if it is greater than or equal to 0.93\n",
    "        if (logs.get('accuracy') >= 0.93): \n",
    "            print(\"\\nAt least 93% training accuracy reached! Training stopped.\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "# class instantiation \n",
    "callbacks = myCallback()\n",
    "\n",
    "# creates new Sequential model \n",
    "def create_model(): \n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)), \n",
    "        Dense(256, activation=\"relu\"), \n",
    "        Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"sgd\", \n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "that_model = create_model()\n",
    "that_model.fit(x_train, \n",
    "               y_train, \n",
    "               epochs = 10,\n",
    "               # use the callbacks parameter to pass the instance above in a list\n",
    "               callbacks = [callbacks]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: Callbacks Exercise\n",
    "\n",
    "Refit *func_model* such that the classifier trains to a training loss of 0.2 or below -- training should stop once training loss is at or below 0.2. \n",
    "\n",
    "Print \"0.2 training loss reached! Training stopped\" when desired loss is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(### your code here ##3): \n",
    "    def on_epoch_end(self, epochs, logs = {}): \n",
    "        ### your code here ###\n",
    "    \n",
    "callbacks = ### your code here ###\n",
    "\n",
    "this_model = create_model()    \n",
    "this_.fit(x_train, y_train, ### your code here###);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Do: Callbacks Debugging\n",
    "\n",
    "The code below attempts to stop training once 97% validation accuracy is reached. However, there seems to be a TypeError. (TypeError: '>=' not supported between instances of 'NoneType' and 'float') Figure out why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not modify myCallback class\n",
    "class myCallback(Callback): \n",
    "    def on_epoch_end(self, epoch, logs = {}): \n",
    "        if (logs.get('val_accuracy') >= 0.97): \n",
    "            print(\"\\nValidation Accuracy of 97% reached. Training Stopped.\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "callbacks = myCallback()\n",
    "fixme_model = create_model()\n",
    "fixme_model.fit(x_train, y_train, epochs=20, callbacks = [callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functional API -- Inception Network\n",
    "\n",
    "As mentioned in section 2, the Functional API allows one to create layers that are impossible with the Sequential API -- ex. parallel layers, concatenations, multiple inputs/ouputs etc. This section explores such layers with functional syntax, and focuses on implementing the following Inception Network.\n",
    "\n",
    "Why an Inception Network? <br>\n",
    "The salient parts of can image can vary greatly in size. For example, an image labeled \"dog\" may have the dog either occupying most of the image, a part of the image, or very little of the image. This makes choosing the right kernel size challenging because smaller kernels are preferred for locally distributed information and larger kernels are preferred for more globally distributed information. <br>\n",
    "The Inception Network allows different filter sizes on the same level, and the results of each filter are concatenated into the next layer. This also makes the network \"wider\" rather than \"deeper\", hence less computationally expensive and prone to overfitting. \n",
    "\n",
    "<img src=\"inception.png\">\n",
    "\n",
    "An arrow leaving layer *x* and pointing towards layer *y* means that layer *y* follows layer *x*. Notice that several arrows leaving from the same layer indicates **branching** and several arrows pointing to the same layer indicates a **concatenation**. Do you notice any repetition in the graph above? The same inception module is applied twice!\n",
    "\n",
    "#### TO DO: Coding the Inception Module\n",
    "The two code cells below implement the inception network in the image above -- fill in *pass* with the correct code. Your clues are all in the image above and code below -- use your knowledge of convolutional neural networks!! There should be 513,120 total trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "def inception_mod(input_layer, br1, br2_in, br2_out, br3_in, br3_out, br4_out):\n",
    "    # 3x3 convolution\n",
    "    conv3 = Conv2D(pass, (1, 1), padding=\"same\", activation=\"relu\")(pass)\n",
    "    conv3 = Conv2D(pass, (3, 3), padding=\"same\", activation=\"relu\")(pass)\n",
    "    # 5x5 convolution \n",
    "    conv5 = Conv2D(pass, (1, 1), padding=\"same\", activation=\"relu\")(pass)\n",
    "    conv5 = Conv2D(pass, (pass, pass), padding = \"same\", activation=\"relu\")(conv5)\n",
    "    # 3x3 max pool\n",
    "    pool3 = MaxPooling2D((3, 3), strides=(1, 1), padding=\"same\")(pass)\n",
    "    pool3 = Conv2D(pass, (1, 1), padding=\"same\", activation=\"relu\")(pass)\n",
    "    # 1x1 convolution\n",
    "    conv1 = Conv2D(pass, (pass, pass), padding=\"same\", activation=\"relu\")(input_layer)\n",
    "    # concatenation\n",
    "    return concatenate([pass, pass, pass, pass], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(256, 256, 3))\n",
    "\n",
    "module1 = inception_mod(pass, 64, 96, 128, 16, 32, 32)\n",
    "module2 = inception_mod(module1, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "model = Model(inputs = pass, outputs = pass)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
